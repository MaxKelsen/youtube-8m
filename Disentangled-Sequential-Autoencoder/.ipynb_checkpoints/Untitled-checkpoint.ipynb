{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# A block consisting of convolution, batch normalization (optional) followed by a nonlinearity (defaults to Leaky ReLU)\n",
    "class ConvUnit(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride=1, padding=0, batchnorm=True, nonlinearity=nn.LeakyReLU(0.2)):\n",
    "        super(ConvUnit, self).__init__()\n",
    "        if batchnorm is True:\n",
    "            self.model = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels, kernel, stride, padding),\n",
    "                    nn.BatchNorm2d(out_channels), nonlinearity)\n",
    "        else:\n",
    "            self.model = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel, stride, padding), nonlinearity)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A block consisting of a transposed convolution, batch normalization (optional) followed by a nonlinearity (defaults to Leaky ReLU)\n",
    "class ConvUnitTranspose(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride=1, padding=0, out_padding=0, batchnorm=True, nonlinearity=nn.LeakyReLU(0.2)):\n",
    "        super(ConvUnitTranspose, self).__init__()\n",
    "        if batchnorm is True:\n",
    "            self.model = nn.Sequential(\n",
    "                    nn.ConvTranspose2d(in_channels, out_channels, kernel, stride, padding, out_padding),\n",
    "                    nn.BatchNorm2d(out_channels), nonlinearity)\n",
    "        else:\n",
    "            self.model = nn.Sequential(nn.ConvTranspose2d(in_channels, out_channels, kernel, stride, padding, out_padding), nonlinearity)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A block consisting of an affine layer, batch normalization (optional) followed by a nonlinearity (defaults to Leaky ReLU)\n",
    "class LinearUnit(nn.Module):\n",
    "    def __init__(self, in_features, out_features, batchnorm=True, nonlinearity=nn.LeakyReLU(0.2)):\n",
    "        super(LinearUnit, self).__init__()\n",
    "        if batchnorm is True:\n",
    "            self.model = nn.Sequential(\n",
    "                    nn.Linear(in_features, out_features),\n",
    "                    nn.BatchNorm1d(out_features), nonlinearity)\n",
    "        else:\n",
    "            self.model = nn.Sequential(\n",
    "                    nn.Linear(in_features, out_features), nonlinearity)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisentangledVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Network Architecture:\n",
    "        PRIOR OF Z:\n",
    "            The prior of z is a Gaussian with mean and variance computed by the LSTM as follows\n",
    "                h_t, c_t = prior_lstm(z_t-1, (h_t, c_t)) where h_t is the hidden state and c_t is the cell state\n",
    "            Now the hidden state h_t is used to compute the mean and variance of z_t using an affine transform\n",
    "                z_mean, z_log_variance = affine_mean(h_t), affine_logvar(h_t)\n",
    "                z = reparameterize(z_mean, z_log_variance)\n",
    "            The hidden state has dimension 512 and z has dimension 32\n",
    "\n",
    "        CONVOLUTIONAL ENCODER:\n",
    "            The convolutional encoder consists of 4 convolutional layers with 256 layers and a kernel size of 5 \n",
    "            Each convolution is followed by a batch normalization layer and a LeakyReLU(0.2) nonlinearity. \n",
    "            For the 3,64,64 frames (all image dimensions are in channel, width, height) in the sprites dataset the following dimension changes take place\n",
    "            \n",
    "            3,64,64 -> 256,64,64 -> 256,32,32 -> 256,16,16 -> 256,8,8 (where each -> consists of a convolution, batch normalization followed by LeakyReLU(0.2))\n",
    "\n",
    "            The 8,8,256 tensor is unrolled into a vector of size 8*8*256 which is then made to undergo the following tansformations\n",
    "            \n",
    "            8*8*256 -> 4096 -> 2048 (where each -> consists of an affine transformation, batch normalization followed by LeakyReLU(0.2))\n",
    "\n",
    "        APPROXIMATE POSTERIOR FOR f:\n",
    "            The approximate posterior is parameterized by a bidirectional LSTM that takes the entire sequence of transformed x_ts (after being fed into the convolutional encoder)\n",
    "            as input in each timestep. The hidden layer dimension is 512\n",
    "\n",
    "            Then the features from the unit corresponding to the last timestep of the forward LSTM and the unit corresponding to the first timestep of the \n",
    "            backward LSTM (as shown in the diagram in the paper) are concatenated and fed to two affine layers (without any added nonlinearity) to compute\n",
    "            the mean and variance of the Gaussian posterior for f\n",
    "\n",
    "        APPROXIMATE POSTERIOR FOR z (FACTORIZED q)\n",
    "            Each x_t is first fed into an affine layer followed by a LeakyReLU(0.2) nonlinearity to generate an intermediate feature vector of dimension 512,\n",
    "            which is then followed by two affine layers (without any added nonlinearity) to compute the mean and variance of the Gaussian Posterior of each z_t\n",
    "\n",
    "            inter_t = intermediate_affine(x_t)\n",
    "            z_mean_t, z_log_variance_t = affine_mean(inter_t), affine_logvar(inter_t)\n",
    "            z = reparameterize(z_mean_t, z_log_variance_t)\n",
    "\n",
    "        APPROXIMATE POSTERIOR FOR z (FULL q)\n",
    "            The vector f is concatenated to each v_t where v_t is the encodings generated for each frame x_t by the convolutional encoder. This entire sequence  is fed into a bi-LSTM\n",
    "            of hidden layer dimension 512. Then the features of the forward and backward LSTMs are fed into an RNN having a hidden layer dimension 512. The output h_t of each timestep\n",
    "            of this RNN transformed by two affine transformations (without any added nonlinearity) to compute the mean and variance of the Gaussian Posterior of each z_t\n",
    "\n",
    "            g_t = [v_t, f] for each timestep\n",
    "            forward_features, backward_features = lstm(g_t for all timesteps)\n",
    "            h_t = rnn([forward_features, backward_features])\n",
    "            z_mean_t, z_log_variance_t = affine_mean(h_t), affine_logvar(h_t)\n",
    "            z = reparameterize(z_mean_t, z_log_variance_t)\n",
    "\n",
    "        CONVOLUTIONAL DECODER FOR CONDITIONAL DISTRIBUTION p(x_t | f, z_t)\n",
    "            The architecture is symmetric to that of the convolutional encoder. The vector f is concatenated to each z_t, which then undergoes two subsequent\n",
    "            affine transforms, causing the following change in dimensions\n",
    "            \n",
    "            256 + 32 -> 4096 -> 8*8*256 (where each -> consists of an affine transformation, batch normalization followed by LeakyReLU(0.2))\n",
    "\n",
    "            The 8*8*256 tensor is reshaped into a tensor of shape 256,8,8 and then undergoes the following dimension changes \n",
    "\n",
    "            256,8,8 -> 256,16,16 -> 256,32,32 -> 256,64,64 -> 3,64,64 (where each -> consists of a transposed convolution, batch normalization followed by LeakyReLU(0.2)\n",
    "            with the exception of the last layer that does not have batchnorm and uses tanh nonlinearity)\n",
    "\n",
    "    Hyperparameters:\n",
    "        f_dim: Dimension of the content encoding f. f has the shape (batch_size, f_dim)\n",
    "        z_dim: Dimension of the dynamics encoding of a frame z_t. z has the shape (batch_size, frames, z_dim) \n",
    "        frames: Number of frames in the video. \n",
    "        hidden_dim: Dimension of the hidden states of the RNNs \n",
    "        nonlinearity: Nonlinearity used in convolutional and deconvolutional layers, defaults to LeakyReLU(0.2)\n",
    "        in_size: Height and width of each frame in the video (assumed square)\n",
    "        step: Number of channels in the convolutional and deconvolutional layers\n",
    "        conv_dim: The convolutional encoder converts each frame into an intermediate encoding vector of size conv_dim, i.e,\n",
    "                  The initial video tensor (batch_size, frames, num_channels, in_size, in_size) is converted to (batch_size, frames, conv_dim)\n",
    "        factorised: Toggles between full and factorised posterior for z as discussed in the paper\n",
    "\n",
    "    Optimization:\n",
    "        The model is trained with the Adam optimizer with a learning rate of 0.0002, betas of 0.9 and 0.999, with a batch size of 25 for 200 epochs\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, f_dim=256, z_dim=32, conv_dim=2048, step=256, in_size1=1524, in_size2=1058, hidden_dim=512,\n",
    "                 frames=8, nonlinearity=None, factorised=False, device=torch.device('cpu')):\n",
    "        super(DisentangledVAE, self).__init__()\n",
    "        self.device = device\n",
    "        self.f_dim = f_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.frames = frames\n",
    "        self.conv_dim = conv_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.factorised = factorised\n",
    "        self.step = step\n",
    "        self.in_size1 = in_size1\n",
    "        self.in_size2 = in_size2\n",
    "        nl = nn.LeakyReLU(0.2) if nonlinearity is None else nonlinearity\n",
    "\n",
    "        # Prior of content is a uniform Gaussian and prior of the dynamics is an LSTM\n",
    "        self.z_prior_lstm = nn.LSTMCell(self.z_dim, self.hidden_dim)\n",
    "        self.z_prior_mean = nn.Linear(self.hidden_dim, self.z_dim)\n",
    "        self.z_prior_logvar = nn.Linear(self.hidden_dim, self.z_dim)\n",
    "        # POSTERIOR DISTRIBUTION NETWORKS\n",
    "        # -------------------------------\n",
    "        self.f_lstm = nn.LSTM(self.conv_dim, self.hidden_dim, 1,\n",
    "                              bidirectional=True, batch_first=True)\n",
    "        # TODO: Check if only one affine transform is sufficient. Paper says distribution is parameterised by LSTM\n",
    "        self.f_mean = LinearUnit(self.hidden_dim * 2, self.f_dim, False)\n",
    "        self.f_logvar = LinearUnit(self.hidden_dim * 2, self.f_dim, False)\n",
    "\n",
    "        if self.factorised is True:\n",
    "            # Paper says : 1 Hidden Layer MLP. Last layers shouldn't have any nonlinearities\n",
    "            self.z_inter = LinearUnit(self.conv_dim, self.hidden_dim, batchnorm=False)\n",
    "            self.z_mean = nn.Linear(self.hidden_dim, self.z_dim)\n",
    "            self.z_logvar = nn.Linear(self.hidden_dim, self.z_dim)\n",
    "        else:\n",
    "            # TODO: Check if one affine transform is sufficient. Paper says distribution is parameterised by RNN over LSTM. Last layer shouldn't have any nonlinearities\n",
    "            self.z_lstm = nn.LSTM(self.conv_dim + self.f_dim, self.hidden_dim, 1, bidirectional=True, batch_first=True)\n",
    "            self.z_rnn = nn.RNN(self.hidden_dim * 2, self.hidden_dim, batch_first=True)\n",
    "            # Each timestep is for each z so no reshaping and feature mixing\n",
    "            self.z_mean = nn.Linear(self.hidden_dim, self.z_dim)\n",
    "            self.z_logvar = nn.Linear(self.hidden_dim, self.z_dim)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "                ConvUnit(1, step, 5, 1, 2), # 3*64*64 -> 256*64*64\n",
    "                ConvUnit(step, step, 5, 2, 2), # 256,64,64 -> 256,32,32\n",
    "                ConvUnit(step, step, 5, 2, 2), # 256,32,32 -> 256,16,16\n",
    "                ConvUnit(step, step, 5, 2, 2), # 256,16,16 -> 256,8,8\n",
    "                )\n",
    "        self.final_conv_size = in_size1 // 8\n",
    "        self.conv_fc = nn.Sequential(LinearUnit(step * (self.final_conv_size ** 2), self.conv_dim * 2),\n",
    "                LinearUnit(self.conv_dim * 2, self.conv_dim))\n",
    "\n",
    "        self.deconv_fc = nn.Sequential(LinearUnit(self.f_dim + self.z_dim, self.conv_dim * 2, False),\n",
    "                LinearUnit(self.conv_dim * 2, step * (self.final_conv_size ** 2), False))\n",
    "        self.deconv = nn.Sequential(\n",
    "                ConvUnitTranspose(step, step, 5, 2, 2, 1),\n",
    "                ConvUnitTranspose(step, step, 5, 2, 2, 1),\n",
    "                ConvUnitTranspose(step, step, 5, 2, 2, 1),\n",
    "                ConvUnitTranspose(step, 1, 5, 1, 2, 0, nonlinearity=nn.Tanh()))\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 1)\n",
    "            elif isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "    # If random sampling is true, reparametrization occurs else z_t is just set to the mean\n",
    "    def sample_z(self, batch_size, random_sampling=True):\n",
    "        z_out = None # This will ultimately store all z_s in the format [batch_size, frames, z_dim]\n",
    "        z_means = None\n",
    "        z_logvars = None\n",
    "\n",
    "        # All states are initially set to 0, especially z_0 = 0\n",
    "        z_t = torch.zeros(batch_size, self.z_dim, device=self.device)\n",
    "        z_mean_t = torch.zeros(batch_size, self.z_dim, device=self.device)\n",
    "        z_logvar_t = torch.zeros(batch_size, self.z_dim, device=self.device)\n",
    "        h_t = torch.zeros(batch_size, self.hidden_dim, device=self.device)\n",
    "        c_t = torch.zeros(batch_size, self.hidden_dim, device=self.device)\n",
    "\n",
    "        for _ in range(self.frames):\n",
    "            h_t, c_t = self.z_prior_lstm(z_t, (h_t, c_t))\n",
    "            z_mean_t = self.z_prior_mean(h_t)\n",
    "            z_logvar_t = self.z_prior_logvar(h_t)\n",
    "            z_t = self.reparameterize(z_mean_t, z_logvar_t, random_sampling)\n",
    "            if z_out is None:\n",
    "                # If z_out is none it means z_t is z_1, hence store it in the format [batch_size, 1, z_dim]\n",
    "                z_out = z_t.unsqueeze(1)\n",
    "                z_means = z_mean_t.unsqueeze(1)\n",
    "                z_logvars = z_logvar_t.unsqueeze(1)\n",
    "            else:\n",
    "                # If z_out is not none, z_t is not the initial z and hence append it to the previous z_ts collected in z_out\n",
    "                z_out = torch.cat((z_out, z_t.unsqueeze(1)), dim=1)\n",
    "                z_means = torch.cat((z_means, z_mean_t.unsqueeze(1)), dim=1)\n",
    "                z_logvars = torch.cat((z_logvars, z_logvar_t.unsqueeze(1)), dim=1)\n",
    "\n",
    "        return z_means, z_logvars, z_out\n",
    "\n",
    "\n",
    "    def encode_frames(self, x):\n",
    "        # The frames are unrolled into the batch dimension for batch processing such that x goes from\n",
    "        # [batch_size, frames, channels, size, size] to [batch_size * frames, channels, size, size]\n",
    "        x = x.view(-1, 1, self.in_size1, self.in_size2)\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, self.step * (self.final_conv_size ** 2))\n",
    "        x = self.conv_fc(x)\n",
    "        # The frame dimension is reintroduced and x shape becomes [batch_size, frames, conv_dim]\n",
    "        # This technique is repeated at several points in the code\n",
    "        x = x.view(-1, self.frames, self.conv_dim)\n",
    "        return x\n",
    "\n",
    "    def decode_frames(self, zf):\n",
    "        x = self.deconv_fc(zf)\n",
    "        x = x.view(-1, self.step, self.final_conv_size, self.final_conv_size)\n",
    "        x = self.deconv(x)\n",
    "        return x.view(-1, self.frames, 1, self.in_size1, self.in_size2)\n",
    "\n",
    "    def reparameterize(self, mean, logvar, random_sampling=True):\n",
    "        # Reparametrization occurs only if random sampling is set to true, otherwise mean is returned\n",
    "        if random_sampling is True:\n",
    "            eps = torch.randn_like(logvar)\n",
    "            std = torch.exp(0.5*logvar)\n",
    "            z = mean + eps*std\n",
    "            return z\n",
    "        else:\n",
    "            return mean\n",
    "\n",
    "    def encode_f(self, x):\n",
    "        lstm_out, _ = self.f_lstm(x)\n",
    "        # The features of the last timestep of the forward RNN is stored at the end of lstm_out in the first half, and the features\n",
    "        # of the \"first timestep\" of the backward RNN is stored at the beginning of lstm_out in the second half\n",
    "        # For a detailed explanation, check: https://gist.github.com/ceshine/bed2dadca48fe4fe4b4600ccce2fd6e1\n",
    "        backward = lstm_out[:, 0, self.hidden_dim:2 * self.hidden_dim]\n",
    "        frontal = lstm_out[:, self.frames - 1, 0:self.hidden_dim]\n",
    "        lstm_out = torch.cat((frontal, backward), dim=1)\n",
    "        mean = self.f_mean(lstm_out)\n",
    "        logvar = self.f_logvar(lstm_out)\n",
    "        return mean, logvar, self.reparameterize(mean, logvar, self.training)\n",
    "\n",
    "    def encode_z(self, x, f):\n",
    "        if self.factorised is True:\n",
    "            features = self.z_inter(x)\n",
    "        else:\n",
    "            # The expansion is done to match the dimension of x and f, used for concatenating f to each x_t\n",
    "            f_expand = f.unsqueeze(1).expand(-1, self.frames, self.f_dim)\n",
    "            lstm_out, _ = self.z_lstm(torch.cat((x, f_expand), dim=2))\n",
    "            features, _ = self.z_rnn(lstm_out)\n",
    "        mean = self.z_mean(features)\n",
    "        logvar = self.z_logvar(features)\n",
    "        return mean, logvar, self.reparameterize(mean, logvar, self.training)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_mean_prior, z_logvar_prior, _ = self.sample_z(x.size(0), random_sampling=self.training)\n",
    "        conv_x = self.encode_frames(x)\n",
    "        f_mean, f_logvar, f = self.encode_f(conv_x)\n",
    "        z_mean, z_logvar, z = self.encode_z(conv_x, f)\n",
    "        f_expand = f.unsqueeze(1).expand(-1, self.frames, self.f_dim)\n",
    "        zf = torch.cat((z, f_expand), dim=2)\n",
    "        recon_x = self.decode_frames(zf)\n",
    "        return f_mean, f_logvar, f, z_mean, z_logvar, z, z_mean_prior, z_logvar_prior, recon_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "#from model import *\n",
    "from tqdm import *\n",
    "from dataset import *\n",
    "\n",
    "__all__ = ['loss_fn', 'Trainer']\n",
    "\n",
    "\n",
    "def loss_fn(original_seq,recon_seq,f_mean,f_logvar,z_post_mean,z_post_logvar, z_prior_mean, z_prior_logvar):\n",
    "    \"\"\"\n",
    "    Loss function consists of 3 parts, the reconstruction term that is the MSE loss between the generated and the original images\n",
    "    the KL divergence of f, and the sum over the KL divergence of each z_t, with the sum divided by batch_size\n",
    "\n",
    "    Loss = {mse + KL of f + sum(KL of z_t)} / batch_size\n",
    "    Prior of f is a spherical zero mean unit variance Gaussian and the prior of each z_t is a Gaussian whose mean and variance\n",
    "    are given by the LSTM\n",
    "    \"\"\"\n",
    "    batch_size = original_seq.size(0)\n",
    "    mse = F.mse_loss(recon_seq,original_seq,reduction='sum');\n",
    "    kld_f = -0.5 * torch.sum(1 + f_logvar - torch.pow(f_mean,2) - torch.exp(f_logvar))\n",
    "    z_post_var = torch.exp(z_post_logvar)\n",
    "    z_prior_var = torch.exp(z_prior_logvar)\n",
    "    kld_z = 0.5 * torch.sum(z_prior_logvar - z_post_logvar + ((z_post_var + torch.pow(z_post_mean - z_prior_mean, 2)) / z_prior_var) - 1)\n",
    "    return (mse + kld_f + kld_z)/batch_size, kld_f/batch_size, kld_z/batch_size\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self,model,train,test,trainloader,testloader, test_f_expand,\n",
    "                 epochs=100,batch_size=100,learning_rate=0.001,nsamples=1,sample_path='./sample',\n",
    "                 recon_path='./recon/', transfer_path = './transfer/', \n",
    "                 checkpoints='model.pth', style1='image1.sprite', style2='image2.sprite', device=torch.device('cuda:0')):\n",
    "        self.trainloader = trainloader\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.testloader = testloader\n",
    "        self.start_epoch = 0\n",
    "        self.epochs = epochs\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.model = model\n",
    "        self.model.to(device)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.checkpoints = checkpoints\n",
    "        self.optimizer = optim.Adam(self.model.parameters(),self.learning_rate)\n",
    "        self.samples = nsamples\n",
    "        self.sample_path = sample_path\n",
    "        self.recon_path = recon_path\n",
    "        self.transfer_path = transfer_path\n",
    "        self.test_f_expand = test_f_expand\n",
    "        self.epoch_losses = []\n",
    "\n",
    "        #self.image1 = torch.load(self.transfer_path + 'image1.sprite')['sprite']\n",
    "        #self.image2 = torch.load(self.transfer_path + 'image2.sprite')['sprite']\n",
    "        #self.image1 = self.image1.to(device)\n",
    "        #self.image2 = self.image2.to(device)\n",
    "        #self.image1 = torch.unsqueeze(self.image1,0)\n",
    "        #self.image2= torch.unsqueeze(self.image2,0)\n",
    "    \n",
    "    def save_checkpoint(self,epoch):\n",
    "        torch.save({\n",
    "            'epoch' : epoch+1,\n",
    "            'state_dict' : self.model.state_dict(),\n",
    "            'optimizer' : self.optimizer.state_dict(),\n",
    "            'losses' : self.epoch_losses},\n",
    "            self.checkpoints)\n",
    "        \n",
    "    def load_checkpoint(self):\n",
    "        try:\n",
    "            print(\"Loading Checkpoint from '{}'\".format(self.checkpoints))\n",
    "            checkpoint = torch.load(self.checkpoints)\n",
    "            self.start_epoch = checkpoint['epoch']\n",
    "            self.model.load_state_dict(checkpoint['state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            self.epoch_losses = checkpoint['losses']\n",
    "            print(\"Resuming Training From Epoch {}\".format(self.start_epoch))\n",
    "        except:\n",
    "            print(\"No Checkpoint Exists At '{}'.Start Fresh Training\".format(self.checkpoints))\n",
    "            self.start_epoch = 0\n",
    "\n",
    "    def sample_frames(self,epoch):\n",
    "        with torch.no_grad():\n",
    "            pass\n",
    "            _,_,test_z = self.model.sample_z(1, random_sampling=False)\n",
    "            print(test_z.shape)\n",
    "            print(self.test_f_expand.shape)\n",
    "            test_zf = torch.cat((test_z, self.test_f_expand), dim=2)\n",
    "            recon_x = self.model.decode_frames(test_zf) \n",
    "            recon_x = recon_x.view(self.samples*8,1,1524,1058)\n",
    "            torchvision.utils.save_image(recon_x,'%s/epoch%d.png' % (self.sample_path,epoch))\n",
    "    \n",
    "    def recon_frame(self,epoch,original):\n",
    "        with torch.no_grad():\n",
    "            pass\n",
    "            _,_,_,_,_,_,_,_,recon = self.model(original) \n",
    "            image = torch.cat((original,recon),dim=0)\n",
    "            image = image.view(2*8,3,64,64)\n",
    "            os.makedirs(os.path.dirname('%s/epoch%d.png' % (self.recon_path,epoch)),exist_ok=True)\n",
    "            torchvision.utils.save_image(image,'%s/epoch%d.png' % (self.recon_path,epoch))\n",
    "\n",
    "    def style_transfer(self,epoch):\n",
    "        with torch.no_grad():\n",
    "            conv1 = self.model.encode_frames(self.image1)\n",
    "            conv2 = self.model.encode_frames(self.image2)\n",
    "            _,_,image1_f = self.model.encode_f(conv1)\n",
    "            image1_f_expand = image1_f.unsqueeze(1).expand(-1,self.model.frames,self.model.f_dim)\n",
    "            _,_,image1_z = self.model.encode_z(conv1,image1_f)\n",
    "            _,_,image2_f = self.model.encode_f(conv2)\n",
    "            image2_f_expand = image2_f.unsqueeze(1).expand(-1,self.model.frames,self.model.f_dim)\n",
    "            _,_,image2_z = self.model.encode_z(conv2,image2_f)\n",
    "            image1swap_zf = torch.cat((image2_z,image1_f_expand),dim=2)\n",
    "            image1_body_image2_motion = self.model.decode_frames(image1swap_zf)\n",
    "            image1_body_image2_motion = torch.squeeze(image1_body_image2_motion,0)\n",
    "            image2swap_zf = torch.cat((image1_z,image2_f_expand),dim=2)\n",
    "            image2_body_image1_motion = self.model.decode_frames(image2swap_zf)\n",
    "            image2_body_image1_motion = torch.squeeze(image2_body_image1_motion,0)\n",
    "            image1 = torch.squeeze(self.image1, 0)\n",
    "            image2 = torch.squeeze(self.image2, 0)\n",
    "            #os.makedirs(os.path.dirname('%s/epoch%d/image1_body_image2_motion.png' % (self.transfer_path,epoch)),exist_ok=True)\n",
    "            #torchvision.utils.save_image(image1,'%s/epoch%d/image1.png' % (self.transfer_path,epoch))\n",
    "            #torchvision.utils.save_image(image2,'%s/epoch%d/image2.png' % (self.transfer_path,epoch))\n",
    "            #torchvision.utils.save_image(image1_body_image2_motion,'%s/epoch%d/image1_body_image2_motion.png' % (self.transfer_path,epoch))\n",
    "            #torchvision.utils.save_image(image2_body_image1_motion,'%s/epoch%d/image2_body_image1_motion.png' % (self.transfer_path,epoch))\n",
    "\n",
    "    def train_model(self):\n",
    "        self.model.train()\n",
    "        for epoch in range(self.start_epoch,self.epochs):\n",
    "            losses = []\n",
    "            kld_fs = []\n",
    "            kld_zs = []\n",
    "            print(\"Running Epoch : {}\".format(epoch+1))\n",
    "            for i,dataitem in tqdm(enumerate(self.trainloader,1)):\n",
    "                #_,_,_,_,_,_,data = dataitem\n",
    "                data = dataitem\n",
    "                data = data.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                f_mean, f_logvar, f, z_post_mean, z_post_logvar, z, z_prior_mean, z_prior_logvar, recon_x = self.model(data)\n",
    "                loss, kld_f, kld_z = loss_fn(data, recon_x, f_mean, f_logvar, z_post_mean, z_post_logvar, z_prior_mean, z_prior_logvar)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                losses.append(loss.item())\n",
    "                kld_fs.append(kld_f.item())\n",
    "                kld_zs.append(kld_z.item())\n",
    "            meanloss = np.mean(losses)\n",
    "            meanf = np.mean(kld_fs)\n",
    "            meanz = np.mean(kld_zs)\n",
    "            self.epoch_losses.append(meanloss)\n",
    "            print(\"Epoch {} : Average Loss: {} KL of f : {} KL of z : {}\".format(epoch+1,meanloss, meanf, meanz))\n",
    "            self.save_checkpoint(epoch)\n",
    "            self.model.eval()\n",
    "            self.sample_frames(epoch+1)\n",
    "            _,_,_,_,_,_,sample = self.test[int(torch.randint(0,len(self.test),(1,)).item())]\n",
    "            sample = torch.unsqueeze(sample,0)\n",
    "            sample = sample.to(self.device)\n",
    "            #self.sample_frames(epoch+1)\n",
    "            #self.recon_frame(epoch+1,sample)\n",
    "            #self.style_transfer(epoch+1)\n",
    "            self.model.train()\n",
    "        print(\"Training is complete\")\n",
    "\n",
    "sprite = hmm2[:100]\n",
    "sprite_test = hmm2[100:118]\n",
    "batch_size = 1\n",
    "loader = torch.utils.data.DataLoader(sprite, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "device = torch.device('cuda:1')\n",
    "vae = DisentangledVAE(f_dim=256, z_dim=32, step=256, factorised=True,device=device)\n",
    "test_f = torch.rand(1,256, device=device)\n",
    "test_f = test_f.unsqueeze(1).expand(1, 8, 256)\n",
    "trainer = Trainer(vae, sprite, sprite_test, loader ,None, test_f,batch_size=25, epochs=500, learning_rate=0.0002, device=device)\n",
    "trainer.load_checkpoint()\n",
    "trainer.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open('../hmm2.txt','rb') as f:\n",
    "    hmm2 = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(748, 1058)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm2[102].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2995, 1058)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tenvidX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 256])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[1, 1, 8, 256]}, size=[1, 8, 256]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-59241eafa3ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.FloatTensor{[1, 1, 8, 256]}, size=[1, 8, 256]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (4)"
     ]
    }
   ],
   "source": [
    "test_f.unsqueeze(1).expand(1, 8, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (testing)",
   "language": "python",
   "name": "testing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
